<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>William Thong</title>
  
  <meta name="author" content="William Thong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:10px">
      <td style="padding:10px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:10px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:left">
              <post-title>William <bold>Thong<bold></post-title><br><br>
              </p>
              <p>I am a Machine Learning Engineer at <a href="https://machinelearning.apple.com/" target="\_blank">Apple Zurich</a>, with a focus on data curation, model evaluation and generative models in computer vision.
                Previously, I was a Research Scientist at <a href="https://ai.sony/" target="\_blank">Sony AI Zurich</a>, where I led and managed the local team on Responsible AI.
              </p>
              <p>I have led research projects, and helped to scale research teams, in computer vision and related fields.
                My research outcomes have been published at top-tier research venues,
                and have been covered in <a href="https://www.technologyreview.com/2023/09/25/1080191/these-new-tools-could-make-ai-vision-systems-less-biased/" target="\_blank">MIT Tech Review</a>, <a href="https://www.theverge.com/2023/10/4/23902728/ai-bias-skin-tone-hue-sony-research-artificial-intelligence-ethics" target="\_blank">The Verge</a>, <a href="https://www.wired.com/story/ai-algorithms-are-biased-against-skin-with-yellow-hues/" target="\_blank">Wired</a>, among others.
              </p>
              <p>Prior to that, I did my PhD at the <a href="https://ivi.fnwi.uva.nl/vislab/" target="\_blank">Video & Image Sense</a> lab of the <a href="https://www.uva.nl/en" target="\_blank">University of Amsterdam</a>, under the supervision of <a href="http://www.ceessnoek.info/" target="\_blank">Cees Snoek</a>.
                My PhD dissertation involved visual similarity, learning with limited labels, and model biases.
                I was fortunate to be partially funded by multiple scholarships from <a href="https://www.nserc-crsng.gc.ca/index_eng.asp" target="\_blank">NSERC</a>.
              </p>
              <p style="text-align:left">
                [<a href="https://ch.linkedin.com/in/twuilliam">LinkedIn</a>] &nbsp
                [<a href="https://scholar.google.nl/citations?user=CPuz5p4AAAAJ&hl">Google Scholar</a>] &nbsp
                [<a href="https://twitter.com/twuilliam">Twitter</a>] &nbsp
                [<a href="https://github.com/twuilliam">Github</a>]
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/wt-circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Conference &amp; Journal publicatons</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/andrews2023ethical.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/ad3ebc951f43d1e9ed20187a7b5bc4ee-Abstract-Datasets_and_Benchmarks.html">
                <papertitle>Ethical Considerations for Responsible Data Curation</papertitle>
              </a>
              <br>
              Jerone Andrews, Dora Zhao, <strong>William Thong</strong>, Apostolos Modas, Orestis Papakyriakopoulos, Alice Xiang
              <br>
              <em>Neural Information Processing Systems Datasets and Benchmarks (NeurIPS D&B)</em>, 2023
              <br>
              [<a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/ad3ebc951f43d1e9ed20187a7b5bc4ee-Abstract-Datasets_and_Benchmarks.html">paper</a>]&nbsp;
              [<a href="https://arxiv.org/abs/2302.03629">arxiv</a>]&nbsp;
              [<a href="https://github.com/SonyResearch/responsible_data_curation">code</a>]&nbsp;
              <p>We lay out considerations and recommendations for responsible curation of computer vision datasets.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/thong2023beyond.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Thong_Beyond_Skin_Tone_A_Multidimensional_Measure_of_Apparent_Skin_Color_ICCV_2023_paper.pdf">
                <papertitle>Beyond Skin Tone: A Multidimensional Measure of Apparent Skin Color</papertitle>
              </a>
              <br>
              <strong>William Thong</strong>, Przemyslaw Joniak, Alice Xiang
              <br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2023
              <br>
              [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Thong_Beyond_Skin_Tone_A_Multidimensional_Measure_of_Apparent_Skin_Color_ICCV_2023_paper.pdf">paper</a>]&nbsp;
              [<a href="https://arxiv.org/abs/2309.05148">arxiv</a>]&nbsp;
              [<a href="https://github.com/SonyResearch/apparent_skincolor">code</a>]&nbsp;
              <p>We measure apparent skin color, beyond a unidimensional scale with the luminance and hue angle.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/papakyriakopoulos2023augmented.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3593013.3594049">
                <papertitle>Augmented Datasheets for Speech Datasets and Ethical Decision-Making</papertitle>
              </a>
              <br>
              Orestis Papakyriakopoulos, Anna Seo Gyeong Choi, Jerone Andrews, Rebecca Bourke, <strong>William Thong</strong>, Dora Zhao, Alice Xiang, Allison Koenecke
              <br>
              <em>ACM Conference on Fairness, Accountability, and Transparency (FAccT)</em>, 2023
              <br>
              [<a href="https://dl.acm.org/doi/abs/10.1145/3593013.3594049">paper</a>]&nbsp;
              [<a href="https://arxiv.org/abs/2305.04672">arxiv</a>]&nbsp;
              [<a href="https://github.com/SonyResearch/project_ethics_augmented_datasheets_for_speech_datasets">code</a>]&nbsp;
              <p>We augment datasheets to encourage ethical considerations in speech datasets.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/thong2022iqa.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://bmvc2022.mpi-inf.mpg.de/0244.pdf">
                <papertitle>Content-Diverse Comparisons improve IQA</papertitle>
              </a>
              <br>
              <strong>William Thong</strong>, Jose-Costa Pereira, Sarah Parisot, Ales Leonardis, Steven McDonagh
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2022
              <br>
              [<a href="https://bmvc2022.mpi-inf.mpg.de/0244.pdf">paper</a>]&nbsp;
              [<a href="https://arxiv.org/abs/2211.05215">arxiv</a>]&nbsp;
              [<a href="https://github.com/huawei-noah/noah-research/tree/master/aipq">code</a>]&nbsp;
              <p>We enrich image comparisons for learning a perceptual quality metric.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/thong2021diverse.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://doi.org/10.1145/3461646">
                <papertitle>Diversely-Supervised Visual Product Search</papertitle>
              </a>
              <br>
              <strong>William Thong</strong> and
              Cees G. M. Snoek
              <br>
              <em>ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</em>, 2022
              <br>
              [<a href="https://doi.org/10.1145/3461646">paper</a>]&nbsp;
              [<a href="javascript:void(0)">arxiv</a>]&nbsp;
              [<a href="https://github.com/twuilliam/diverse-search">code</a>]&nbsp;
              <p>We create a diverse set of labels from instance, attribute and category similarities for visual product search.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/thong2021bias.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2110.14336">
                <papertitle>Feature and Label Embedding Spaces Matter in Addressing Image Classifier Bias</papertitle>
              </a>
              <br>
              <strong>William Thong</strong>,
              Cees G. M. Snoek
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2021
              <br>
              [<a href="https://www.bmvc2021-virtualconference.com/assets/papers/0180.pdf">paper</a>]&nbsp;
              [<a href="https://arxiv.org/abs/2110.14336">arxiv</a>]&nbsp;
              [<a href="https://github.com/twuilliam/bias-classifiers">code</a>]&nbsp;
              <p>
              We identify and mitigate biases in both feature and label embedding spaces in image classifiers.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/mettes2021object.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2104.04715">
                <papertitle>Object Priors for Classifying and Localizing Unseen Actions</papertitle>
              </a>
              <br>
              Pascal Mettes,
              <strong>William Thong</strong>,
              Cees G. M. Snoek
              <br>
              <em>International Journal of Computer Vision (IJCV)</em>, 2021
              <br>
              [<a href="https://doi.org/10.1007/s11263-021-01454-y">paper</a>]&nbsp;
              [<a href="https://arxiv.org/abs/2104.04715">arxiv</a>]&nbsp;
              [<a href="https://github.com/psmmettes/object-priors-unseen-actions">code</a>]&nbsp;
              <p>We derive spatial and semantic priors to recognize unseen actions in videos with zero training sample.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/thong2020gzsl.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.bmvc2020-conference.com/conference/papers/paper_0261.html">
                <papertitle>Bias-Awareness for Zero-Shot Learning the Seen and Unseen</papertitle>
              </a>
              <br>
              <strong>William Thong</strong> and
              Cees G. M. Snoek
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2020
              <br>
              [<a href="https://www.bmvc2020-conference.com/assets/papers/0261.pdf">paper</a>]&nbsp;
              [<a href="https://arxiv.org/abs/2008.11185">arxiv</a>]&nbsp;
              [<a href="https://github.com/twuilliam/bias-gzsl">code</a>]&nbsp;
              [<a href="https://www.bmvc2020-conference.com/conference/papers/paper_0261.html">video</a>]&nbsp;
              <p>We mitigate the classifier bias towards classes seen during training in generalized zero-shot learning.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/thong2019open.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1911.08621">
                <papertitle>Open Cross-Domain Visual Search</papertitle>
              </a>
              <br>
              <strong>William Thong</strong>, Pascal Mettes, Cees G.M. Snoek
              <br>
              <em>Computer Vision and Image Understanding (CVIU)</em>, 2020
              <br>
              [<a href="https://doi.org/10.1016/j.cviu.2020.103045">paper</a>]&nbsp;
              [<a href="https://arxiv.org/abs/1911.08621">arxiv</a>]&nbsp;
              [<a href="https://github.com/twuilliam/open-search">code</a>]&nbsp;
              <p>We search for categories from any source domain to any target domain in a common semantic space.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/turkoglu2019aaai.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1902.00671">
                <papertitle>A Layer-Based Sequential Framework for Scene Generation with GANs</papertitle>
              </a>
              <br>
              Mehmet O. Turkoglu, <strong>William Thong</strong>, Luuk Spreeuwers, Berkay Kicanaoglu
              <br>
              <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2019
              <br>
              [<a href="https://doi.org/10.1609/aaai.v33i01.33018901">paper</a>]&nbsp;
              [<a href="https://arxiv.org/abs/1902.00671">arxiv</a>]&nbsp;
              [<a href="https://drive.google.com/open?id=1MJhVce9a5jWI6GnW45k4gNFGe-Jie0-z">poster</a>]&nbsp;
              [<a href="https://github.com/0zgur0/Seq_Scene_Gen">code</a>]
              <p>We compose a scene layer-by-layer, with an explicit control over the generation of all scene elements.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/thong2018kidneys.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://doi.org/10.1080/21681163.2016.1148636">
                <papertitle>Convolutional Networks for Kidney Segmentation in Contrast-Enhanced CT Scans</papertitle>
              </a>
              <br>
              <strong>William Thong</strong>, Samuel Kadoury, Nicolas Pich&eacute;, Christopher J. Pal
              <br>
              <em>CMBBE: Imaging &amp; Visualization</em>, 2018
              <br>
            [<a href="https://doi.org/10.1080/21681163.2016.1148636">paper</a>] &ndash; initially presented at <a href="https://cs.adelaide.edu.au/~dlmia/index.html">MICCAI-DLMIA 2015</a>
              <p>We segment healthy and abnormal kidneys in CT scans with a patch-based ConvNet.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/thong2016spine.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://doi.org/10.1007/s00586-016-4426-3">
                <papertitle>Three-dimensional Morphology Study of Surgical Adolescent Idiopathic Scoliosis Patient from Encoded Geometric Models</papertitle>
              </a>
              <br>
              <strong>William Thong</strong>, Stefan Parent, James Wu, Carl-&Eacute;ric Aubin, Hubert Labelle, Samuel Kadoury
              <br>
              <em>European Spine Journal (ESJ)</em>, 2016
              <br>
              [<a href="https://doi.org/10.1007/s00586-016-4426-3">paper</a>]
              <p>We cluster scoliotic spine deformations in 3D representations with a stacked auto-encoder.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/ullmann2014vertebrae.png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://dx.doi.org/10.1155/2014/719520">
                  <papertitle>Automatic Labeling of Vertebral Levels using a Robust Template-Based Approach</papertitle>
              </a>
              <br>
              Eug&eacute;nie Ullmann, Jean Fran&ccedil;ois Pelletier Paquette*, <strong>William Thong</strong>*, Julien Cohen-Adad
              <br>
              <em>International Journal of Biomedical Imaging (IJBI)</em>, 2014
              <br>
              [<a href="http://dx.doi.org/10.1155/2014/719520">paper</a>]
              <p></p>
              <p>We build a template to predict vertebral levels in MRI images.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-top:20px"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Workshop &amp; Abstract publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <a href="https://r2hcai.github.io/AAAI-23/files/CameraReadys/21.pdf">
                  <papertitle>A case study in fairness evaluation: Current limitations and challenges for human pose estimation</papertitle>
              </a>
              <br>
              LaChance* and <strong>Thong*</strong> et al. <em>AAAI workshops</em>, 2023.
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2105.03072">
                  <papertitle>NTIRE 2021 challenge on perceptual image quality assessment</papertitle>
              </a>
              <br>
              Gu et al. <em>CVPR workshops</em>, 2021.
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <a href="https://doi.org/10.1145/3343031.3350597">
                  <papertitle>Interactive Exploration of Journalistic Video Footage through Multimodal Semantic Matching</papertitle>
              </a>
              <br>
              Ibrahimi et al. <em>ACM Multimedia (Demo track)</em>, 2019.
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <a href="https://doi.org/10.1007/978-3-319-14148-0_2">
              <papertitle>Stacked Auto-Encoders for Classification of 3D Spine Models in Adolescent Idiopathic Scoliosis</papertitle>
              </a>
              <br>
              <strong>Thong</strong> et al. <em>MICCAI-CSI workshop</em>, 2014.
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <a href="https://www.researchgate.net/publication/284653428_Spinal_Cord_Toolbox_an_open-source_framework_for_processing_spinal_cord_MRI_data">
                  <papertitle>Spinal Cord Toolbox: an Open-Source Framework for Processing Spinal Cord MRI Data</papertitle>
              </a>
              <br>
              Cohen-Adad et al. <em>OHBM</em>, 2014.
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Academic service</heading>
              <p>
              Reviewer for CVPR, ECCV, ICCV, NeurIPS.
              </p>
              <p>
              Outstanding reviewer awards at CVPR'21 and BMVC'20 and 21.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:10px">
              <br>
              <p style="text-align:right;font-size:small;">
                Webpage template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>

</body>

</html>
